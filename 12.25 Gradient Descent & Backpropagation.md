## 梯度消失和梯度爆炸
梯度消失指在反向传播过程中，梯度趋近于零，导致深层网络几乎无法更新
梯度爆炸则相反，梯度变得极大，导致权重更新过大，网络无法收敛。这两个问题的本质都是神经网络层数过深，导致链式求导的连乘效应被放大。
在反向传播中，需要使用链式求导计算损失函数对每层参数的偏导，假设n层的网络，其梯度计算简化为
$$\frac{\partial J}{\partial w_i}=\sum_m\cdot \cdot\cdot\sum_l\sum_j\frac{\partial z_m^{(1)}}{\partial w_i}\cdot \cdot\cdot \frac{\partial z_j^{(n)}}{\partial z_l^{(n-1)}}\frac{\partial J}{\partial z_j^{(n)}}$$
其中$z_j^{(k)}$代表第k层第j个节点的激活值。如果连乘项中大部分元素小于1或者大于1，则会导致梯度趋于0或无穷。
缓解这一问题的方法有多种，主要包括：
- 使用ReLU等非饱和（导数几乎不为0）的激活函数
- 恰当的参数初始化
- 使用归一化
- 残差连接
- 梯度剪切


## 参数初始化

梯度下降要求在开始时选择合适的参数初始值，这将影响参数收敛的速度；目前参数初始化方法较少，核心思想是打破对称性，使得不同神经元经过不同的权重可以输出不同的激活值。朴素的初始化方法是从$[-\epsilon, \epsilon]$范围的均匀分布或者服从$N(0,\epsilon^2)$的正态分布中随机采样；
而随后研究表明保证**神经网络中每一层输入与输出的方差一致**，可以有效减少梯度问题，出现了 **LeCun 初始化、Xavier 初始化和 He 初始化**。

### LeCun初始化
该初始化使用于Sigmoid、Tanh等近似线性的激活函数，考虑神经网络某一层输入与输出满足：$$y=\sum_{i=1}^{n_{in}}w_ix_i$$
则经计算有$$Var(y)=\sum_{i=1}^{n_{in}}Var(w_i)Var(x_i)=n_{in}Var(w)Var(x)$$
因此为了满足输入与输出的方差一致，有$$Var(w)=\frac{1}{n_{in}}$$

### Xavier 初始化
该方法在LeCun 初始化的基础上考虑反向传播的方差，由于反向传播时与权重矩阵转置$W^T$有关，要求$n_{out}Var(w)=1$，由于一般$n_{in}$不等于$n_{out}$，采用调和平均作为权重的方差
$$Var(w)=\frac{2}{n_{in}+n_{out}}$$

### He 初始化
上述两种初始化方法要求激活函数在线性区工作，对于 ReLU 激活函数失效，He 初始化主要用于解决使用 ReLU 及其变体作为激活函数时梯度消失或梯度爆炸的问题，其核心思想是保证保持每一层输出的方差与输入的方差一致；
假设神经网络的第l层服从下列的转换：
$$a_i^{(l)}=\sum_{j=1}^{M}w_{ij}z_j^{(l-1)}$$
$$z_i^{(l)}=ReLU(a_i^{(l)})$$
其中M代表与第i个神经元连接的上一层神经元个数。
假设第l-1层的激活值$z_j^{(l-1)}$方差为$\lambda^2$，初始化权重均值为0，则根据概率论可以计算得到：
$$
E[a_i^{(l)}]=0 
$$
$$
Var[z_j^{(l)}]=\frac{M}{2}Var[w_{ij}]\lambda^2
$$
$\frac{1}{2}$从 ReLU 的二阶矩中得来，为了保证方差一致，要求$Var[W]=\sqrt{\frac{2}{M}}$ 。如果初始化权重服从正态分布$N(0,\epsilon^2)$，则$\epsilon=\sqrt{\frac{2}{M}}$；如果服从均匀分布$U[-\epsilon,\epsilon]$，则$\epsilon=\sqrt{\frac{6}{M}}$。

### 正交初始化
对于循环神经网络，梯度消失和梯度爆炸的问题更严重，为此需要采用正交初始化；其思想是将权重矩阵初始化为正交矩阵（极端情况下初始化为单位矩阵），这样可以梯度在长距离传播中保持模长不变，显著提升处理长序列的能力。

#### 疑问
- 为什么保证输入与输出的方差一致就能稳定梯度？
	损失函数关于权重的梯度和关于输入/输出的梯度相关
- 为什么 He 初始化不需要类似 Xavier 初始化考虑反向传播？
	$n_{in}$与$n_{out}$区别较小，且大多数层不存在差异，累积差异不会导致指数级变化
- 能否通过某种初始化方法使得训练中更易收敛到目标函数全局最小值点？


## 归一化

为了稳定梯度，减轻梯度消失、梯度爆炸的影响，除了正确的初始化方法以外，还可以在训练过程中进行归一化
常用的归一化包括：**数据归一化、批次归一化以及层归一化**等。

### 数据归一化
输入神经网络的数据的不同变量之间往往具有不同的取值范围，这种差异会对梯度下降带来挑战，为此需要在使用数据之前进行归一化预处理，主要将不同变量缩放到统一的取值范围，如$$\hat{x}_{ni}=\frac{x_{ni}-\mu_i}{\sigma_i}$$
其中$n$代表样本序号，$i$代表变量序号，$\mu_i$和$\sigma_i$分别代表所有数据样本中第i个变量的均值和方差。

### 批次归一化
批次归一化与数据归一化类似，但是针对的是神经网络中隐藏层变量，即在每次训练权重更新后，将批次中全部样本对应的隐藏层值进行归一化，同样可以采用上述的正交归一化方法。为了进一步增强隐藏层的表征能力，需要通过额外的参数将隐藏层状态的范围变换到合适的范围，即：
$$
\hat{a}_{ni}=\frac{a_{ni}-\mu_i}{\sqrt{\sigma_i^2+\delta}}
$$
$$
\bar{a}_{ni}=\gamma_i \hat{a}_{ni}+\beta_i
$$
其中$\mu_i,\sigma_i$分别是批次内样本之间隐藏态的均值与方差，$\delta$是小量。

训练过程中可以通过批次内多个样本计算归一化所需要的均值和方差，但是模型推理阶段，一次只输入单个样本，无法直接根据输入进行归一化；为此，需要在训练过程中记录每一个批次归一化的均值以及方差，此外为了增加训练末期归一化设置的权重，采用移动平均：
$$
\bar{\mu}_i^{(\tau)}=\alpha\bar{\mu}_i^{(\tau-1)}+(\alpha-1)\mu_i
$$
$$
\bar{\sigma}_i^{(\tau)}=\alpha\bar{\sigma}_i^{(\tau-1)}+(\alpha-1)\sigma_i
$$
- **Q**:$\gamma,\beta$在什么时候更新？
- **A**: 前向传播时进行

$$\text{Input} \xrightarrow{\text{Forward}} \underbrace{\text{Conv} \rightarrow \mathbf{BN} \rightarrow \text{ReLU}}_{\text{计算 Loss}} \xrightarrow{\text{Backward}} \underbrace{\text{Calc Gradients}}_{\text{算出 } \Delta W, \Delta \gamma, \Delta \beta} \xrightarrow{\text{Optimizer}} \underbrace{\text{Update Weights}}_{\text{更新 } W, \gamma, \beta}$$
在激活函数前归一化有利于利用激活函数的非线性，防止梯度消失，保证进入激活函数的数据分布稳定。

### 层归一化
层归一化指对单个批次以及同一隐藏层内不同的神经元之间进行归一化，同样可以引入额外的可学习参数对归一化之后的值进行变换；与批次归一化不同的是，层归一化可以在推理阶段对单个样本进行，因此不需要再训练阶段存储归一化的参数。归一化形式如下：
$$
\hat{a}_{ni}=\frac{a_{ni}-\mu_n}{\sqrt{\sigma_n^2+\delta}}
$$

对于Transformer等架构，更适合采用层归一化，原因如下：
- 输入序列的长度不一致，需要padding，会影响BN的效果；
- BN需要批次大小较大，而Transformer架构参数量大导致难以实现较大批次；
- 训练与推理阶段的数据差异较大，BN训练中得到的归一化统计量不匹配。

层归一化的顺序包括后归一化（Post-Norm）和预归一化（Pre-Norm），训练倾向于 Pre-Norm，即先进归一化，之后进行注意力等计算。

1. 后归一化
LN 位于残差加法之后：
$$x_{l+1}=LayerNorm(x_l+F_l(x_l))$$
其中$F(x_l)$代表自注意力或前馈网络子层；
后归一化相对更难训练，需要严苛的学习率预热。
$$\frac{\partial \mathcal{L}}{\partial x_l} = \frac{\partial \mathcal{L}}{\partial x_{l+1}} \cdot \underbrace{\frac{\partial \text{LayerNorm}(y_l)}{\partial y_l}}_{\text{① LN 的雅可比矩阵}} \cdot \underbrace{\left( I + \frac{\partial F_l(x_l)}{\partial x_l} \right)}_{\text{② 残差结构的导数}}$$
$$\frac{\partial \text{LayerNorm}}{\partial y_l} \approx \frac{\gamma}{\sigma_l} \cdot (\dots)$$
第$l$层梯度来自于下一层梯度乘上系数，那么层数加深，系数如果偏离1，梯度会指数缩小或放大，导致梯度消失或爆炸。

2. 预归一化
LN 位于子层计算之前，而残差连接直接跳过子层和 LN：
$$x_{l+1}=x_{l}+F_l(LayerNorm(x_l))$$
$$\frac{\partial \mathcal{L}}{\partial x_l} = \frac{\partial \mathcal{L}}{\partial x_{l+1}} \cdot \left( I + \frac{\partial F_l}{\partial \text{LN}} \frac{\partial \text{LN}}{\partial x_l} \right)$$
假设每一层更新$F_l$方差近似为1，那么随着层数增加，$x_L$的方差会线性累积，深层网络中$\frac{\partial \text{LN}}{\partial x_l}\approx \frac{\gamma}{\sigma_L}$ 会很小，梯度更新主要来源于恒等映射，可能导致“表征崩塌”。
3. DeepNorm
$$x_{l+1} = \text{LayerNorm}(\alpha \cdot x_l + f_l(x_l, \theta_l))$$
引入系数$\alpha > 1$放大主干信号$x_l$，$f_l(x_l,\theta_l)$保持不变或变小，整体采用后归一化结构，避免表征崩塌。
工程实现通过缩放初始化时的权重，即
$$W_{int}\leftarrow \frac{W_{std}}{\beta}$$
对于N层的Transformer，参数一般取为$\alpha=(2N)^{1/4},\beta=(8N)^{1/4}$。

## 计算图
### 定义
计算图是一个有向图$G=(V,E)$，用于表示数学表达式的运算过程。
1. 节点
在通用的计算图定义中，节点通常代表数学运算（Operations, Ops）或变量（Variables）。
- **运算节点**：代表函数变换，如矩阵乘法（MatMul）、加法（Add）、激活函数（ReLU, Sigmoid）等。
- **变量节点**：代表承载数据的实体，如标量、向量、矩阵或张量（Tensor）。
2. 边
 图中的有向边 $u \to v$ 表示节点 $u$ 的输出是节点 $v$ 的输入。这种连接关系确立了两个核心属性：
- **数据依赖（Data Dependency）**：为了计算节点 $v$ 的值，必须先计算节点 $u$ 的值。这隐含了拓扑排序（Topological Sort）的需求，决定了前向传播（Forward Pass）的执行顺序。
- **梯度传播路径（Gradient Flow Path）**：在反向传播（Backward Pass）中，梯度沿着边的反方向流动。边上的权重或雅可比矩阵（Jacobian Matrix）定义了局部导数，通过链式法则在图中累积。

### 静态图与动态图
1. 静态图
静态图指先构建完整的计算图，然后执行数值计算，典型实现为 TensorFlow 1.x。该模式由于在代码执行前已知图结构，可以进行全局优化，但是同样会导致调试困难。
2. 动态图
动态图指计算图在代码运行过程中同步构建，每次前向传播均动态生成一个计算图。这将带来更高的灵活性，但是存在一定的运行资源要求。

### PyTorch 动态计算图


## 自动微分
### 自动微分的必要性
使用梯度反向传播更新参数时，需要计算损失函数关于参数的梯度值，共有四种方式计算。
- 第一种方法是提前计算出反向传播中梯度的表达式，然后在代码中显式定义并计算，该方法依赖人力计算梯度表达式，对于目前的深层网络不适用。
- 第二种方法称为符号微分，使用计算机代数等方法自动计算梯度的解析表达式，该方法解决了人力的问题，但是符号微分得到的解析式往往很复杂，同时存在重复计算浪费资源的问题；此外，符号微分要求需要被微分的表达式具有封闭形式（即不包括无穷级数、极限、积分或迭代求解过程）。
- 第三种方法采用有限差分对梯度进行数值计算，但是该方法存在计算精度不足问题，由于计算单个参数的梯度均需要一次前向传播，所以对于参数量大的网络难以规模化应用；但是该方法不依赖反向传播过程，可以用于检验反向传播算法的正确性。
- 第四种方法是自动微分，包括前向自动微分和反向自动微分两种，其依赖计算图逐步计算梯度值，是目前深度学习的主流算法。

### 数值微分
数值微分方法基于有限差分计算梯度，通过对权重进行微扰，计算目标函数的变化近似求解梯度，公式如下：
$$
\frac{\partial J_n}{\partial w_{ji}}=\frac{J_n(w_{ji}+\epsilon)-J_n(w_{ji})}{\epsilon}+O(\epsilon)
$$
其中$\epsilon$为小量，误差与$\epsilon$大小成正比；此外可以通过采用中心差分进一步提高精度至$O(\epsilon^2)$
$$
\frac{\partial J_n}{\partial w_{ji}}=\frac{J_n(w_{ji}+\epsilon)-J_n(w_{ji}-\epsilon)}{2\epsilon}+O(\epsilon^2)
$$
减小$\epsilon$可以有效提高精度，直至达到计算机数值极限。
该方法只需要正向传播计算损失函数，对于每个权重的梯度计算，需要$O(W)$步，因此整个网络的权重计算代价为$O(W^2)$。
但是数值微分的结果可以用来和自动微分等其他依赖反向传播的结果对比，用于检查代码正确性。

### 反向自动微分
计算图构建之后，假设中间变量为$v_i$，输出函数为$f$，则定义$\bar{v}_i=\frac{\partial f}{\partial v_i}$ ，因此根据链式法则有
$$\bar{v}_i=\frac{\partial f}{\partial v_i}=\sum_{j\in ch(i)}\frac{\partial f}{\partial v_j}\frac{\partial v_j}{\partial v_i}=\sum_{j\in ch(i)}\bar{v}_j\frac{\partial v_j}{\partial v_i}$$
因此沿着计算图反向传播，即可计算出输出值关于输入值的微分，每个输出值对应一次传播路径，对于$M$个输出变量，只需要$M$条反向传播路径，即可得到整个雅可比矩阵。
但是由于反向传播计算过程需要使用正向传播的中间变量值，因此需要存储正向传播的中间变量值，有一定的内存要求。
### 正向自动微分
正向自动微分与反向类似，假设中间变量$v_i$，输入变量为$x_1$，输出函数$f$，不同的是定义中间变量关于输入的微分$\dot{v}_i=\frac{\partial v_i}{\partial x_1}$ ，同样根据链式求导有
$$\dot{v}_i=\frac{\partial v_i}{\partial x_1}=\sum_{j\in pa(i)}\frac{\partial v_j}{\partial x_1}\frac{\partial v_i}{\partial v_j}=\sum_{j\in pa(i)}\dot{v}_j\frac{\partial v_i}{\partial v_j}$$
其中$pa(i)$代表i节点的父节点，即指向i节点。
沿着计算图正向传播过程中，可以同步的计算$\dot{v}_i$值，最终得到$\dot{f}$值；输出关于每个输入变量的微分均需要一次独立正向传播路径，即假设有N个输入变量，计算关于第i个输入变量的微分，需要设置$\dot{x}_i=1,\dot{x}_j=0（j\neq i）$，进而需要N次正向传播才能够得到整个雅各比矩阵，计算资源要求高。

### 实际应用
一般情况下，输入变量远多于输出变量(即$N>>M$)，正向自动微分尽管能够同步计算中间状态值与微分，但需要过多的正向传播过程，计算资源要求过高；因此在实际应用中一般采用反向自动微分。

但是前向自动微分在某些场景也具有优势，比如计算雅可比矩阵和向量的积
$$
\mathbf{J} = 
\begin{bmatrix}
    \frac{\partial f_1}{\partial x_1} & \cdots & \frac{\partial f_1}{\partial x_D} \\
    \vdots & \ddots & \vdots \\
    \frac{\partial f_K}{\partial x_1} & \cdots & \frac{\partial f_K}{\partial x_D}
\end{bmatrix}
\begin{bmatrix}
    r_1 \\
    \vdots \\
    r_D
\end{bmatrix}
$$
即可通过设置$\dot{\textbf{x}}=\textbf{r}$，进行一次前向传播计算得到。

混合自动微分可以用于计算海森矩阵与向量的乘积$\textbf{Hv}$，首先设置$\dot{\textbf{x}}=\textbf{v}$并进行一次正向传播得到$y = (\nabla f(x))\textbf{v}$（y为标量），再对于上述计算过程进行反向传播，得到$\nabla_x (\nabla f(x) \textbf{v})=\nabla^2 f \textbf{v}=\textbf{Hv}$ ， 总的时间复杂度为$O(W)$。